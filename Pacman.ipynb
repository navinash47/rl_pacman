{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Create Pacman environment with deterministic settings\n",
    "def make_environment(difficulty=0, mode=0, repeat_action_probability=0.0, frameskip=4):\n",
    "    env = gym.make(\n",
    "        \"ALE/Pacman-v5\",\n",
    "        render_mode=\"rgb_array\",\n",
    "        difficulty=0,  # Easiest difficulty\n",
    "        mode=0,        # Default mode\n",
    "        repeat_action_probability=0.0,  # Fully deterministic\n",
    "        frameskip=4,   # Fixed frameskip\n",
    "    )\n",
    "    return env\n",
    "\n",
    "# Environment specifications:\n",
    "# - Discrete action space with 5 actions (0:NOOP, 1:UP, 2:RIGHT, 3:LEFT, 4:DOWN)\n",
    "# - RGB observation space (210, 160, 3)\n",
    "# - Fixed frameskip of 4 frames\n",
    "# - Deterministic environment with 0.25 repeat action probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = make_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# Test function to run episodes\n",
    "\n",
    "def test_environment(env, num_episodes=2, max_steps=100, render=True):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        print(f\"Initial Info: {info}\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Random action\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Take step\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Render if requested\n",
    "            if render:\n",
    "                # Display the game screen\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(observation)\n",
    "                plt.axis('off')\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "                plt.close()\n",
    "                time.sleep(0.1)  # Add delay to make it viewable\n",
    "            \n",
    "            # Print step information\n",
    "            # print(f\"Step {step + 1}: Action={action}, Reward={reward}, Done={terminated or truncated}\")\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "        print(f\"Episode {episode + 1} finished with total reward: {total_reward}\")\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "    return all_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(5)\n",
      "Observation Space: Box(0, 255, (250, 160, 3), uint8)\n",
      "Action Meanings: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN']\n",
      "\n",
      "Episode 1\n",
      "Initial Info: {'lives': 4, 'episode_frame_number': 16, 'frame_number': 32}\n",
      "Episode 1 finished with total reward: 5.0\n",
      "\n",
      "Episode 2\n",
      "Initial Info: {'lives': 4, 'episode_frame_number': 16, 'frame_number': 448}\n",
      "Episode 2 finished with total reward: 5.0\n",
      "[5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "# Print environment information\n",
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Meanings:\", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# Run test\n",
    "all_rewards = test_environment(env, num_episodes=2, max_steps=100, render=False)\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess state function\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Convert RGB image to grayscale and resize to smaller dimensions\"\"\"\n",
    "    # Convert to grayscale and normalize\n",
    "    gray = np.mean(state, axis=2)  # Convert RGB to grayscale\n",
    "    # Resize to smaller dimensions (e.g., 84x84)\n",
    "    resized = np.resize(gray, (84, 84))\n",
    "    # Flatten and normalize\n",
    "    processed = resized.flatten() / 255.0\n",
    "    return processed\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=7056, action_dim=5):  # 84x84 = 7056\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        if IN_COLAB:\n",
    "            self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if IN_COLAB:\n",
    "            x = x.to(device)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the value network (baseline)\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=7056):  # 84x84 = 7056\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        if IN_COLAB:\n",
    "            self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if IN_COLAB:\n",
    "            x = x.to(device)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Function to select action\n",
    "def select_action(policy_net, state):\n",
    "    processed_state = preprocess_state(state)\n",
    "    state = torch.from_numpy(processed_state).float().unsqueeze(0)\n",
    "    if IN_COLAB:\n",
    "        state = state.to(device)\n",
    "    probs = policy_net(state)\n",
    "    action = np.random.choice(len(probs[0]), p=probs.cpu().detach().numpy()[0])\n",
    "    return action\n",
    "\n",
    "# Function to compute returns\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "# Main training loop\n",
    "@torch.cuda.amp.autocast() if IN_COLAB else lambda x: x\n",
    "def train(env, policy_net, value_net, policy_optimizer, value_optimizer, num_episodes=1000):\n",
    "    # Create scaler for mixed precision training if on Colab\n",
    "    scaler = torch.cuda.amp.GradScaler() if IN_COLAB else None\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        \n",
    "        # Generate an episode\n",
    "        while True:\n",
    "            action = select_action(policy_net, state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            processed_state = preprocess_state(state)\n",
    "            state_tensor = torch.from_numpy(processed_state).float().unsqueeze(0)\n",
    "            if IN_COLAB:\n",
    "                state_tensor = state_tensor.to(device)\n",
    "            \n",
    "            log_prob = torch.log(policy_net(state_tensor)[0][action])\n",
    "            value = value_net(state_tensor)\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            \n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = compute_returns(rewards)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "        if IN_COLAB:\n",
    "            returns = returns.to(device)\n",
    "        values = torch.cat(values)\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_loss = -torch.sum(torch.stack(log_probs) * advantages)\n",
    "        policy_optimizer.zero_grad()\n",
    "        if IN_COLAB:\n",
    "            scaler.scale(policy_loss).backward()\n",
    "            scaler.step(policy_optimizer)\n",
    "        else:\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "        \n",
    "        # Update value network\n",
    "        value_loss = nn.functional.mse_loss(values, returns)\n",
    "        value_optimizer.zero_grad()\n",
    "        if IN_COLAB:\n",
    "            scaler.scale(value_loss).backward()\n",
    "            scaler.step(value_optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % 1 == 0:\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {sum(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 17.0\n",
      "Episode 2, Total Reward: 8.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m value_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Close the environment\u001b[39;00m\n\u001b[1;32m     16\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, policy_net, value_net, policy_optimizer, value_optimizer, num_episodes)\u001b[0m\n\u001b[1;32m    100\u001b[0m value_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(values, returns)\n\u001b[1;32m    101\u001b[0m value_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 102\u001b[0m \u001b[43mvalue_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m value_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize environment and networks\n",
    "\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "# action_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork()\n",
    "value_net = ValueNetwork()\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the agent\n",
    "train(env, policy_net, value_net, policy_optimizer, value_optimizer, num_episodes=1000)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
