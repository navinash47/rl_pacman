\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{times}
\usepackage{fancyhdr} % Required for header and footer
\usepackage{algorithm} % Required for algorithm environment
\usepackage{algpseudocode} % Required for algorithmic environment

\pagestyle{fancy}
\fancyhf{} % Clear default header/footer
% \rhead{687-project}
\lhead{COMPSCI 687 - Reinforcement Learning}
% \rfoot{Page \thepage} % align center
\cfoot{\thepage}
% \lfoot{anandyala@umass.edu, svavilapalli@umass.edu}

\title{Monte Carlo Tree Search (MCTS), REINFORCE with baseline and Actor Critic}
\author{anandyala@umass.edu svavilapalli@umass.edu }
\begin{document}

\maketitle
\thispagestyle{fancy}

\section{Introduction}

In this project, we have implemented three influential algorithmic approaches in reinforcement learning: a) Monte Carlo Tree Search (MCTS), b) REINFORCE with baseline, and c) Actor-Critic methods. 
Each algorithm represents different paradigms in solving sequential decision-making problems.

Monte Carlo Tree Search (MCTS) stands out as a powerful decision-time planning algorithm that has revolutionized artificial intelligence, particularly in game-playing domains. 
Most notably, MCTS was instrumental in advancing computer Go from amateur level in 2005 to grandmaster level by 2015, forming the foundation for AlphaGo's historic achievements.
MCTS combines tree search with Monte Carlo sampling, making it particularly effective in domains with large state spaces.

REINFORCE with baseline, a policy gradient method, represents a different approach to reinforcement learning. 
By incorporating a baseline function, it addresses the high variance issues common in basic policy gradient methods. 
This algorithm learns both a policy and a value function, with the latter serving to reduce variance in gradient estimates while maintaining unbiased updates.

Actor-Critic methods bridge the gap between value-based and policy-based approaches. 
By maintaining separate networks for policy (actor) and value function (critic), these methods combine the advantages of both paradigms. 
The critic's value estimates help reduce variance in the actor's policy updates while maintaining the ability to learn stochastic policies.

We evaluated these algorithms across three distinct environments:
\begin{itemize}
    \item The 687-gridworld environment, a classic grid-based navigation task with stochastic transitions and obstacles
    \item The Acrobot environment, a challenging control problem requiring precise joint manipulation
    \item The Cat Vs Monsters environment, a modified grid world featuring dynamic obstacles and multiple threat types
\end{itemize}

In the following sections, we provide detailed descriptions of each environment and algorithm, followed by comprehensive experimental results and comparative analyses. Our evaluations focus on learning efficiency, convergence properties, and final performance across these diverse environments. Through this study, we aim to understand the strengths and limitations of each approach in different types of reinforcement learning problems.

\section{Environments}

\subsection{687-gridworld}

This is a grid world environment with a 5x5 grid. The agent has 4 actions: up, down, left, and right.
The agent starts at the top left corner and the goal is to reach the bottom right corner.
The agent has to avoid obstacles and water.
For transitioning, the agent has an 80 percent chance to perform the intended action, a 5 percent chance to malfunction and move right, a 5 percent chance to move left, and a 10 percent chance to stay in place.
The agent gets a reward of -10 if it reaches water, 10 if it reaches the goal, and 0 for each step taken.
The first state is $S_0 = (0,0)$ and the goal state is $S_{\infty} = (4,4)$. Obstacles are at $(2,2)$ and $(3,2)$. Water is at $(4,2)$.

\subsection{Cat Vs Monsters}

This is a grid world environment with a 5x5 grid.
The cat agent starts at the top left corner and the goal is to reach the bottom right corner.
The cat agent has to avoid the furniture and the monsters. For transitioning, the agent has a 70 percent chance to perform the intended action, a 12 percent chance to malfunction and move right, a 12 percent chance to move left, and an 8 percent chance to stay in place.
The cat agent gets a reward of -8 if it reaches monsters, 10 if it reaches the goal, and -0.05 for each step taken.
The first state is $S_0 = (0,0)$ and the goal state is $S_{\infty} = (4,4)$.
Furniture is at $(2,1)$, $(2,2)$, $(2,3)$, $(3,2)$. Monsters are at $(4,2)$, $(0,3)$.

\subsection{Acrobot}

This is a classic control environment consisting of two links connected linearly to form a chain, with one end fixed.
The agent controls the joint between the two links by applying torque (-1, 0, or 1).
The goal is to swing the free end of the chain above a given height while starting from a hanging downward position.
For each state, the agent observes 6 values: cosine and sine of both joint angles ie., $\theta_1$ and $\theta_2$, and their angular velocities ie., $\omega_1$ and $\omega_2$.
The agent receives a reward of -1 for each step and 0 when reaching the goal height. And reward threshold is -100
The episode terminates when either the goal height is reached which can be calculated mathematically by $-cos(\theta_1)-cos(\theta_1+\theta_2)>1.0$ or it terminates after 500 steps.
The starting state is uniformy distributed by $cos(\theta_1), sin(\theta_1), cos(\theta_2), sin(\theta_2), \omega_1, \omega_2 \in [-0.1,0.1]$.
Constraints are $\theta_1, \theta_2 \in [-\pi, \pi]$ rads, $\omega_1 \in [-4\pi, 4\pi]$ rads/sec, $\omega_2 \in [-9\pi, 9\pi]$ rads/sec.





\section{Algorithms}

\subsection{Monte Carlo Tree Search (MCTS)}

Monte Carlo Tree Search (MCTS) is a decision-time planning algorithm that combines tree search with Monte Carlo sampling to evaluate actions. 
It builds a search tree incrementally by running simulations, focusing computational resources on the most promising lines of play. 
The algorithm is particularly effective in domains with large state spaces where traditional search methods are impractical.

\begin{algorithm}
\caption{Monte Carlo Tree Search}
\begin{algorithmic}[1]
\State Initialize root node with current state
\While{computational budget remains}
    \State \textbf{Selection:}
    \State \hspace{\algorithmicindent} Starting from root, traverse tree using UCT:
    \State \hspace{\algorithmicindent} $UCT(node) = \frac{wins}{visits} + C\sqrt{\frac{\ln(N)}{visits}}$
    \State \hspace{\algorithmicindent} Until reaching either:
    \State \hspace{\algorithmicindent} - Unexpanded node, or
    \State \hspace{\algorithmicindent} - Terminal state
    \State \textbf{Expansion:}
    \State \hspace{\algorithmicindent} If node not terminal:
    \State \hspace{\algorithmicindent} Add one or more child nodes to tree
    \State \textbf{Simulation:}
    \State \hspace{\algorithmicindent} From new node, simulate to terminal state
    \State \hspace{\algorithmicindent} Using random rollout policy
    \State \textbf{Backpropagation:}
    \State \hspace{\algorithmicindent} Update statistics (visits and values of all nodes)
    \State \hspace{\algorithmicindent} For all nodes traversed in this iteration
\EndWhile
\State \Return action with highest visit count at root
\end{algorithmic}
\end{algorithm}

The algorithm balances exploration and exploitation through the UCT (Upper Confidence Bounds for Trees) formula, where $C$ is an exploration constant (typically $\sqrt{2}$), $N$ is the parent node's visit count, and $total\_value/visits$ represents the node's average value.

MCTS has several advantages:
\begin{itemize}
    \item Anytime algorithm: can return a result at any time, with quality improving with more computation
    \item Asymmetric tree growth: focuses search on promising moves
    \item No domain-specific knowledge required beyond game rules
    \item Effective in large state spaces where full tree search is impractical
\end{itemize}

The algorithm has been particularly successful in game-playing domains, most notably in computer Go where it formed the basis for AlphaGo's groundbreaking success.

\subsection{REINFORCE with baseline}

REINFORCE with baseline is an extension of the basic REINFORCE algorithm that introduces a baseline function to reduce variance in policy gradient estimates while maintaining unbiased updates.
The baseline function estimates the state value, which is then subtracted from the returns to create an advantage estimate.

The algorithm updates both a policy network $\pi(a|s,\theta)$ and a baseline value function $\hat{v}(s,\mathbf{w})$ using the following steps:

\begin{algorithm}
\caption{REINFORCE with Baseline}
\begin{algorithmic}[1]
\State Initialize policy parameters $\theta$ and baseline parameters $\mathbf{w}$
\For{each episode}
    \State Generate trajectory following policy $\pi(a|s,\theta)$
    \For{each timestep $t$ in the episode}
        \State Calculate return $G_t = \sum_{k=t+1}^T \gamma^{k-t-1}R_k$
        \State Calculate advantage estimate $\delta = G_t - \hat{v}(S_t,\mathbf{w})$
        \State Update baseline weights: $\mathbf{w} \leftarrow \mathbf{w} + \alpha^w \delta \nabla \hat{v}(S_t,\mathbf{w})$
        \State Update policy parameters: $\theta \leftarrow \theta + \alpha^\theta \gamma^t \delta \nabla \ln \pi(A_t|S_t,\theta)$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

The baseline helps reduce variance in the gradient estimates without biasing them, as the baseline is subtracted from the return before computing the policy gradient.
This typically leads to faster and more stable learning compared to the basic REINFORCE algorithm. 
The algorithm uses two learning rates: $\alpha^w$ for the baseline updates and $\alpha^\theta$ for the policy updates.

\subsection{Actor Critic}

Actor-Critic methods combine policy gradient with value function approximation, where the "Actor" learns a policy and the "Critic" learns to evaluate the policy. Unlike REINFORCE with baseline, Actor-Critic methods use the value function to bootstrap the return estimation, making them fully online and incremental.

The algorithm maintains two networks:
\begin{itemize}
    \item Actor: Policy network $\pi(a|s,\theta)$ that determines action selection
    \item Critic: Value network $\hat{v}(s,\mathbf{w})$ that estimates state values
\end{itemize}

\begin{algorithm}
\caption{Actor-Critic with Eligibility Traces}
\begin{algorithmic}[1]
\State Initialize policy parameters $\theta$ and value parameters $\mathbf{w}$
\For{each episode}
    \State Initialize state $S$
    \State Initialize eligibility traces $\mathbf{z}^\theta = \mathbf{0}$, $\mathbf{z}^w = \mathbf{0}$
    \While{$S$ is not terminal}
        \State Select action $A \sim \pi(\cdot|S,\theta)$
        \State Take action $A$, observe $R$, $S'$
        \State Calculate TD error: $\delta = R + \gamma\hat{v}(S',\mathbf{w}) - \hat{v}(S,\mathbf{w})$
        \State Update critic trace: $\mathbf{z}^w \leftarrow \gamma\lambda^w\mathbf{z}^w + \nabla\hat{v}(S,\mathbf{w})$
        \State Update actor trace: $\mathbf{z}^\theta \leftarrow \gamma\lambda^\theta\mathbf{z}^\theta + \nabla\ln\pi(A|S,\theta)$
        \State Update critic: $\mathbf{w} \leftarrow \mathbf{w} + \alpha^w\delta\mathbf{z}^w$
        \State Update actor: $\theta \leftarrow \theta + \alpha^\theta\delta\mathbf{z}^\theta$
        \State $S \leftarrow S'$
    \EndWhile
\EndFor
\end{algorithmic}
\end{algorithm}

The algorithm uses eligibility traces ($\lambda^w$, $\lambda^\theta$) to handle credit assignment over multiple timesteps. The critic's TD error $\delta$ serves as the advantage estimate for both networks. This online nature makes Actor-Critic methods more data-efficient than Monte Carlo methods like REINFORCE, though they may introduce some bias in the gradient estimates.

The algorithm uses separate step sizes $\alpha^w$ and $\alpha^\theta$ for the critic and actor updates respectively, and separate trace decay rates $\lambda^w$ and $\lambda^\theta$ to control the trade-off between bias and variance in each component.

\section{Implementation and Results}

\subsection{687-gridworld}

\subsection{Acrobot}

\subsection{Cat Vs Monsters}




\section{References}


\end{document}